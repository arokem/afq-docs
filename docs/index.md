# DIRECT : Data-intensive REsearch in ConnecTomics

Human neuroscience is rapidly turning towards data-driven research thanks in
large part to concerted efforts that collect and aggregate large
[openly-available datasets](XXX).

DIRECT is a collaboration focused on data-driven understanding of the human
connectome. We develop and use tools that quantify different aspects of human brain
networks and relate brain network properties to complex behavioral phenotypes.

We develop an ecosystem of [software projects](about/projects.md) developed by DIRECT are:


See the [team](XXX).

If you are interested, feel free to [contact us](XXX).

We are grateful for [funding](XXX) that supports our work.

Want to learn about human connectomics? Read more [here](XXX)


A paragon of this kind of data collection is the NIH-funded
[Human Connectome Project (HCP)](https://www.humanconnectome.org/), which provides high-quality
MRI data from more than 1,200 young healthy individuals, coupled with behavioral
and genetic information [@]. Several other ongoing studies are currently
amassing datasets with many thousands of individuals: [The Healthy Brain Network
study](http://fcon_1000.projects.nitrc.org/indi/cmi_healthy_brain_network/index.html) [@Alexander2017hbn] and [Adolescent Brain Cognitive Development](https://abcdstudy.org/) study [@Jernigan2018abcd] will each collect
cross-sectional and longitudinal data from approximately 10,000 children during
their development, together with detailed information about their cognitive and
mental health development. The [UK Biobank](https://www.ukbiobank.ac.uk/) will
eventually provide access to MRI measurements from approximately 100,000 individuals [@Sudlow2015ukbb].
Many other projects are also collecting very large datasets on a variety of clinical populations.
This trend is not likely to abate, as the field is becoming increasingly aware
that large samples are crucial for reproducibility and for accurate inferences
about the role of brain structure and function in complex behaviors and in
mental and neurological health [@marek2020repro]. In fact, these datasets are already having a
profound impact on neuroscience as data-driven methods are employed to
illuminate the biological underpinnings of human brain health in new and
important ways.


We focus our investigations primarily on the measurements of the white matter
contains the long-range connections between different brain regions. Diffusion
MRI (dMRI) is a powerful imaging technology that provides non-invasive *in vivo*
measurements of these connections. These measurements have provided compelling
evidence that the biophysical properties of these connections account for
individual variation in many important cognitive skills, and they predict
clinical symptoms across a variety of psychiatric and neurological disorders
(reviewed in [@Rokem2017JoVWM] and [@wandell2016clarifying]).



<!--
## TODO: Expectations for autofq site?
Is autofq the central information repository or is it an overview and visitors should go to individual project websites/github pages?

## TODO: What is AFQ?
Could leverage some content from [AFQ wiki](https://github.com/yeatmanlab/AFQ/wiki)

## TODO: Who is AFQ?
Background information on project(s)? Who to contact for questions? Citations for usage?

## TODO: Why is AFQ?
More on motivation. Define intended audiences (e.g., researchers/library users and developers)
-->

<!--
## TODO: What are some intended uses and use cases?
* Incorporate links to samples/examples/tutorials.
* What knowledge is assumed? (e.g., neuroanatomy, neuroinformatics, neuro imaging, tractometry, git, python, AWS).
* Do want to get into providing background resources (e.g., review papers, dictionary terminology)
* Create use case documentation (how to install and run [local vs cloud] vs how to set up development environment and contribute)
-->

This website documents the AFQ projects:

<!--
## TODO: Based on my use case, how do I know which project to use? How do the projects relate/integrate? What are they capable of what are limitations? release schedule information?
## TODO: What is processing pipeline? diagram? what are steps? which are optional? what are tolerances (recommended settings for use cases)?
## TODO: requirements? assumptions? dependencies? integration? interoperability? standards/specifications?
## TODO: How to diagnose or debug output? tests? confirm validity? sanity checks?
-->





\bibliography