# Using Amazon Web Services S3

Data for our projects is stored on the [Amazon Web Services](https://aws.amazon.com/) (AWS)
[Simple Storage Service](https://aws.amazon.com/s3/) (S3).

## Accessing data from HCP

The data from the Human Connectome project is provided as part of
[AWS Open Data program](https://aws.amazon.com/opendata/). The
HCP dataset entry in the program is provided [here](https://registry.opendata.aws/hcp-openaccess/).

To access the processed Human Connectome Project data, use the instructions
provided [here](https://wiki.humanconnectome.org/display/PublicData/How+to+Get+Access+to+the+HCP+OpenAccess+Amazon+S3+Bucket)

To add your HCP credentials to your configuration, you will need to use the
[command-line interface](getting_started.md).

    aws configure --profile hcp

We also have code in pyAFQ that automatically fetches/reads HCP data from S3.

## Uploading data to S3

Before uploading data to our S3 storage, please organize it in a 'BIDSish'
format on your local hard-drive. This will look something like this:

    |    <study>
    |      ├-derivatives
    |            ├-<pipeline>
    |                ├── sub01
    |                │   ├── ses01
    |                │   │   ├── anat
    |                │   │   │   ├── sub-01_ses-01_aparc+aseg.nii.gz
    |                │   │   │   └── sub-01_ses-01_T1.nii.gz
    |                │   │   └── dwi
    |                │   │       ├── sub-01_ses-01_dwi.bvals
    |                │   │       ├── sub-01_ses-01_dwi.bvecs
    |                │   │       └── sub-01_ses-01_dwi.nii.gz
    |                │   └── ses02
    |                │       ├── anat
    |                │       │   ├── sub-01_ses-02_aparc+aseg.nii.gz
    |                │       │   └── sub-01_ses-02_T1w.nii.gz
    |                │       └── dwi
    |                │           ├── sub-01_ses-02_dwi.bvals
    |                │           ├── sub-01_ses-02_dwi.bvecs
    |                │           └── sub-01_ses-02_dwi.nii.gz
    |                └── sub02
    |                   ├── ses01
    |                   │   ├── anat
    |                   │       ├── sub-02_ses-01_aparc+aseg.nii.gz
    |                   │   │   └── sub-02_ses-01_T1w.nii.gz
    |                   │   └── dwi
    |                   │       ├── sub-02_ses-01_dwi.bvals
    |                   │       ├── sub-02_ses-01_dwi.bvecs
    |                   │       └── sub-02_ses-01_dwi.nii.gz
    |                   └── ses02
    |                       ├── anat
    |                       │   ├── sub-02_ses-02_aparc+aseg.nii.gz
    |                       │   └── sub-02_ses-02_T1w.nii.gz
    |                       └── dwi
    |                           ├── sub-02_ses-02_dwi.bvals
    |                           ├── sub-02_ses-02_dwi.bvecs
    |                           └── sub-02_ses-02_dwi.nii.gz


Where `study` will be the name of the study and will also be the name we will
use for the bucket on S3. Instead of `pipeline` use a name of the preprocessing
pipeline that you used to process the data. For example, you might use `vista`,
if you processed the data with the vistalab tools or `dmriprep` if you use
dmriprep.

Here, we will use the command-line interface to upload the data
(see how to [get started](getting_started.md) with that).

The command reference for S3 sub-commands is [here](https://docs.aws.amazon.com/cli/latest/reference/s3/index.html).
Specifically, to create a bucket on S3, you will use the [`mb`](https://docs.aws.amazon.com/cli/latest/reference/s3/mb.html) sub-sub-command:

```
aws s3 mb s3://study
```

This command should be executed once. Then, uploading the data as a sync operation:

```
aws s3 sync path/to/study s3://study
```

The use of `sync` (rather than `aws s3 cp`) means that only new data would be
uploaded (for example, if repeated calls are made).






